
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Customer Churn in the Telecom Sector\n",
    "This notebook analyzes the Telco Customer Churn dataset to build models for predicting customer churn.\n",
    "The dataset is sourced from Kaggle: https://www.kaggle.com/datasets/blastchar/telco-customer-churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "# Make sure the 'Telco-Customer-Churn.csv' file is in the same directory as this notebook.\n",
    "data = pd.read_csv('Telco-Customer-Churn.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "In this section, we clean the data, encode categorical variables, and prepare it for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Handle missing values\n",
    "# Drop rows with missing values to ensure data quality\n",
    "data = data.dropna()\n",
    "\n",
    "# Step 2: Encode categorical variables\n",
    "# Use LabelEncoder to convert categorical features into numeric values\n",
    "label_encoders = {}  # Dictionary to store encoders for potential inverse transformations\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    if column != 'Churn':  # Skip the target variable for now\n",
    "        le = LabelEncoder()\n",
    "        data[column] = le.fit_transform(data[column])\n",
    "        label_encoders[column] = le\n",
    "\n",
    "# Encode the target variable ('Churn')\n",
    "# Convert 'Yes' to 1 and 'No' to 0\n",
    "data['Churn'] = data['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "# Step 3: Separate features and target variable\n",
    "X = data.drop('Churn', axis=1)  # Features\n",
    "y = data['Churn']             # Target variable\n",
    "\n",
    "# Step 4: Standardize numerical features\n",
    "# Use StandardScaler to normalize features for better model performance\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shapes of the training and testing sets\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "In this section, we train a Random Forest Classifier and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(random_state=42)  # Initialize the model\n",
    "model.fit(X_train, y_train)  # Train the model\n",
    "\n",
    "# Step 2: Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 3: Evaluate the model\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Classification Report:\n', classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 4: Plot the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "Identify the most important features influencing customer churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Feature importance analysis\n",
    "# Extract feature importance scores from the Random Forest model\n",
    "feature_importance = model.feature_importances_\n",
    "feature_names = data.drop('Churn', axis=1).columns\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the top 10 most important features\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df.head(10), palette='viridis')\n",
    "plt.title('Top 10 Important Features')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
